Momentum = combDat("momentum"),
Size = combDat("size"),
Vol = combDat("vol"),
Yield = combDat("yield")
)
res
}
combDat = getCombDat()
# Diff data
getDiff = function() {
# difference certain items in the combined dataset to get incremental values
# PX_LAST is treated such that it is the return of week STARTING ...
# other variables are treated such that they are return of week ENDING ...
diffDat = function(df) {
diffItems = c(
# "PX_LAST",
# "Quality",
# "Value",
# "Momentum",
# "Growth",
# "Size",
# "Vol",
# "Yield",
"EEM",
"Housing",
"VIX",
"BCom",
"DXY",
"spx"
)
for (item in diffItems) {
exp = parse(text=paste("df$",item,sep=""))
df[,item] = c(NA,diff(eval(exp)))
}
df$PX_LAST = c(diff(df$PX_LAST),NA)
df = na.omit(df)
df
}
res = list(
Quality = diffDat(combDat$Quality),
Value = diffDat(combDat$Value),
Growth = diffDat(combDat$Growth),
Momentum = diffDat(combDat$Momentum),
Size = diffDat(combDat$Size),
Vol = diffDat(combDat$Vol),
Yield = diffDat(combDat$Yield)
)
res
}
diffDat = getDiff()
# get excess market returns
getDeMarket = function() {
# get excess factor returns and excess sector returns
# the data frame is NOT!!! scaled
deMarket = function(df) {
diffItems = c(
# "Quality",
# "Value",
# "Momentum",
# "Growth",
# "EEM"
)
for (item in diffItems) {
exp1 = parse(text=paste("df$",item,sep=""))
df[,item] = eval(exp1) - df$spx
}
df
}
res = list(
Quality = deMarket(diffDat$Quality),
Value = deMarket(diffDat$Value),
Growth = deMarket(diffDat$Growth),
Momentum = deMarket(diffDat$Momentum),
Size = deMarket(diffDat$Size),
Vol = deMarket(diffDat$Vol),
Yield = deMarket(diffDat$Yield)
)
}
deMarketDat = getDeMarket()
# Smooth data
expSmooth = function(dat) {
#convenience function for polynomially weighted smoothing, half-life is 4
hl = 1
delta = 0.5^(1/hl)
l = length(dat)
weight = rep(NA,l)
for (i in 1:l) {
weight[i] = delta^(l-i)
}
weight = weight/sum(weight)
smoothed = sum(weight * dat)
smoothed
}
smoothDat = function(df) {
# apply quarter-montly smoothing to weekly data; (a quarter ~ 12 weeks)
# the differenced items are now summed because we want cumulative return
# the undifferenced items are now averaged as they are indicators of regime
# the resulting dataframe is NOT!!!!! scaled
diffItems = c(
"PX_LAST",
"EEM",
"Housing",
"VIX",
"BCom",
"DXY",
"spx"
)
nonDiff = setdiff(colnames(df),diffItems)
for (item in diffItems) {
exp = parse(text=paste("df$",item,sep=""))
df[,item] = c(rep(NA,times=11),rollapply(eval(exp),FUN=sum,width = 12))
}
for (item in nonDiff) {
exp = parse(text=paste("df$",item,sep=""))
df[,item] = c(rep(NA,times=11),rollapply(eval(exp),FUN=expSmooth,width = 12))
}
#df = as.data.frame(scale(na.omit(df)))
df = as.data.frame(na.omit(df))
df
}
getSmooth = function() {
res = list(
Quality = smoothDat(deMarketDat$Quality),
Value = smoothDat(deMarketDat$Value),
Growth = smoothDat(deMarketDat$Growth),
Momentum = smoothDat(deMarketDat$Momentum),
Size = smoothDat(deMarketDat$Size),
Vol = smoothDat(deMarketDat$Vol),
Yield = smoothDat(deMarketDat$Yield)
)
}
# smoothDat = getSmooth()
# smoothDat2=getSmooth()
smoothDat3=getSmooth()
getMeanSd = function(fact) {
exp = parse(text=paste("smoothDat3$",fact,"$PX_LAST",sep=""))
mean = mean(eval(exp))
sd = sd(eval(exp))
list(
mean = mean,
sd = sd
)
}
getMeanSd("Quality")
round(res,1)
resScaled = res
resScaled["Quality",] = getMeanSd("Quality")$mean + getMeanSd("Quality")$sd * resScaled["Quality",]
resScaled["Value",] = getMeanSd("Value")$mean + getMeanSd("Value")$sd * resScaled["Value",]
resScaled["Growth",] = getMeanSd("Growth")$mean + getMeanSd("Growth")$sd * resScaled["Growth",]
resScaled["Momentum",] = getMeanSd("Momentum")$mean + getMeanSd("Momentum")$sd * resScaled["Momentum",]
resScaled["Size",] = getMeanSd("Size")$mean + getMeanSd("Size")$sd * resScaled["Size",]
resScaled["Vol",] = getMeanSd("Vol")$mean + getMeanSd("Vol")$sd * resScaled["Vol",]
resScaled["Yield",] = getMeanSd("Yield")$mean + getMeanSd("Yield")$sd * resScaled["Yield",]
resScaled
round(resScaled,3)
vote = function(vec) {
mean(vec[2:4])
}
apply(resScaled,1,vote)
rec
rec = apply(resScaled,1,vote)
rec
quantile(rec)
rec
round(resScaled,3)
A = matrix(c(1,0,6,3,4,15,5,6,21),byrow=TRUE,nrow=3)
det(A)
A = matrix(c(1,-2,4,-2,3,0,4,0,3),byrow=TRUE,nrow=3)
eig = eigen(A)
A = matrix(c(1,-2,4,-2,3,0,4,0,3),byrow=TRUE,nrow=3)
eig = eigen(A)
eig
?eigen
A - 6.59 * identity()
A - 6.59 * identity(x)
x
A - 6.59 * diag(3)
solve(A)
round(solve(A),2)
eigen(solve(A))
require(numDeriv)
?grad
require(numDeriv)
f1 = function(x) {
return(sum(x^3)-3*x[1]*x[2])
}
f2 = function(x) {
return(
x[1]^2+(x[1]+x[2])^2+(x[1]+x[3])^2
)
}
grad(f1,c(0,0))
grad(f1,c(1,1))
grad(f2,c(0,0,0))
?hessian()
require(nloptr)
?cobyla
require(nloptr)
f1 = function(x) {
1+x[2]+x[3]^2
}
hin_ = function(x) {
h = numeric(2)
h[1] = x[1]^2+x[2]^2-1
h[2] = 1-(x[1]^2+x[2]^2)
return(h)
}
S = cobyla(c(0,0,0),f1,hin=hin_)
S$par
require(nloptr)
f1 = function(x) {
1+x[2]+x[3]^2
}
hin_ = function(x) {
h = numeric(2)
h[1] = x[1]^2+x[2]^2-1
h[2] = 1-(x[1]^2+x[2]^2)
return(h)
}
S = cobyla(c(0,0,0),f1,hin=hin_)
S
f1 = function(x) {
2*x[1]^2+x[2]^2
}
hin_ = function(x) {
h = numeric(2)
h[1] = x[1]+x[2]
h[2] = -(x[1]^2+x[2])
return(h)
}
S = cobyla(c(0,0,0),f1,hin=hin_)
S
f1 = function(x) {
2*x[1]^2+x[2]^2
}
hin_ = function(x) {
h = numeric(2)
h[1] = x[1]+x[2]-1
h[2] = 1-(x[1]^2+x[2])
return(h)
}
S = cobyla(c(0,0,0),f1,hin=hin_)
S
f1 = function(x) {
2*x[1]^2+x[2]^2
}
hin_ = function(x) {
h = numeric(2)
h[1] = x[1]+x[2]-1.05
h[2] = 1.05-(x[1]^2+x[2])
return(h)
}
S = cobyla(c(0,0,0),f1,hin=hin_)
S
f1 = function(x) {
x^3-3*x
}
S = cobyla(0,f1,upper=2)
S
f1 = function(x) {
-x^3+3*x
}
S = cobyla(0,f1,upper=2)
S$value = -S$value # Since we changed max problem to min problem
f1 = function(x) {
-x^3+3*x
}
S = cobyla(0,f1,upper=2)
S$value = -S$value # Since we changed max problem to min problem
S
f1 = function(x) {
-x^3+3*x
}
S = cobyla(0,f1,upper=2)
S$value = -S$value # Since we changed max problem to min problem
S
f1 = function(x) {
x^3-3*x
}
S = cobyla(0,f1,upper=2)
S$value = -S$value # Since we changed max problem to min problem
S
f1 = function(x) {
-x^3+3*x
}
S = cobyla(0,f1,upper=2)
S$value = -S$value # Since we changed max problem to min problem
S
rho = c(0.0427,0.0015,0.0285)
rho = c(0.0427,0.0015,0.0285)
C = matrix(c(0.01,0.002,0.001,
0.002,0.011,0.003,
0.001,0.003,0.020),byrow=TRUE,nrow=3)
e = rep(1,times=3)
rho = c(0.0427,0.0015,0.0285)
C = matrix(c(0.01,0.002,0.001,
0.002,0.011,0.003,
0.001,0.003,0.020),byrow=TRUE,nrow=3)
e = rep(1,times=3)
xmv = solve(C) %*% e / (t(e) %*% solve(C) %*% e)
rho = c(0.0427,0.0015,0.0285)
C = matrix(c(0.01,0.002,0.001,
0.002,0.011,0.003,
0.001,0.003,0.020),byrow=TRUE,nrow=3)
e = rep(1,times=3)
xmv = as.vector(solve(C) %*% e) / (t(e) %*% solve(C) %*% e)
xmv
t(e) %*% solve(C) %*% e
(solve(C) %*% e)
as.vector(solve(C) %*% e)
rho = c(0.0427,0.0015,0.0285)
C = matrix(c(0.01,0.002,0.001,
0.002,0.011,0.003,
0.001,0.003,0.020),byrow=TRUE,nrow=3)
e = rep(1,times=3)
xmv = as.vector(solve(C) %*% e) / (t(e) %*% solve(C) %*% e)
xmv
f1 = function(x) {
t(x) %*% C %*% x
}
hin_ = function(x) {
h = numeric(2)
h[1] = t(rho) %*% x - (rho[2]+rho[3])/2
h[2] = -t(rho) %*% x + (rho[2]+rho[3])/2
return(h)
}
S = cobyla(c(0,0,0),f1,hin=hin_)
S
f1 = function(x) {
t(x) %*% C %*% x
}
hin_ = function(x) {
h = numeric(1)
h[1] = t(rho) %*% x - (rho[2]+rho[3])/2
return(h)
}
S = cobyla(c(0,0,0),f1,hin=hin_)
S
f1 = function(x) {
t(x) %*% C %*% x
}
hin_ = function(x) {
h = numeric(1)
h[1] = t(rho) %*% x - (rho[2]+rho[3])/2
return(h)
}
S = cobyla(c(0.3,0.3,0.3),f1,hin=hin_)
S
plot(f.reg,main="Low Vol Factor Return Prediction")
require(forecast)
require(randomForest)
require(neuralnet)
require(caret)
require(RSNNS)
plot(f.reg,main="Low Vol Factor Return Prediction")
lines(202:255,ret.test,col="red",lwd=3)
lines(202:255,f.lasso,col="orange",lwd=2)
lines(202:255,f.lasso,col="orange",lwd=2)
f.lasso
f.lasso = predict(m.train.lasso$model,newx=as.matrix(xreg.test),s=m.train.lasso$bestLam,type="link")
m.train.lasso
f.lasso = predict(m.train.lasso$model,newx=as.matrix(xreg.test),s=m.train.lasso$bestLam,type="link")
lines(202:255,f.rf,col="yellow",lwd=2)
f.rf
f.rf = predict(m.train.rf,newdata=xreg.test)
lines(202:255,f.rf,col="yellow",lwd=2)
lines(202:255,f.rf,col="yellow",lwd=3)
f.rnn = predict(nn2,newdata = xreg.test,type="raw")
lines(202:255,f.rnn,col="green",lwd=2)
head(smoothDat$IT)
head(df)
head(df[test,])
tail(df[train,])
seq.Date(from=as.Date("2015-08-19"),by=week,length=255)
seq.Date(from=as.Date("2015-08-19"),by="week",length=255)
seq.Date(from=as.Date("2003-08-19"),by="week",length=255)
seq.Date(from=as.Date("2007-08-19"),by="week",length=255)
seq.Date(from=as.Date("2008-08-19"),by="week",length=255)
seq.Date(from=as.Date("2011-08-19"),by="week",length=255)
rownames(df[train,])
f.lasso = predict(m.train.lasso$model,newx=as.matrix(xreg.test),s=m.train.lasso$bestLam,type="link")
m.train.lasso$model
xreg.test
as.matrix(xreg.test)
m.train.lasso
m.train.lasso$model
m.train.lasso = getWeightedLasso(df,train,f=as.formula("PX_LAST~."))
require(glmnet)
f.lasso = predict(m.train.lasso$model,newx=as.matrix(xreg.test),s=m.train.lasso$bestLam,type="link")
plot(f.reg,main="Low Vol Factor Return Prediction")
lines(202:255,ret.test,col="red",lwd=3)
lines(202:255,f.lasso,col="orange",lwd=2)
lines(202:255,f.rf,col="yellow",lwd=3)
lines(202:255,f.rnn,col="green",lwd=2)
legend(x=0,y=1,"Time Series Regression",col="blue",pch=15)
legend(x=0,y=-0.35,"Lasso Regression",col="orange",pch=15)
legend(x=0,y=1-0.35,"Random Forest",col="yellow",pch=15)
legend(x=0,y=1-0.7,"Regularized Neural Net",col="green",pch=15)
legend(x=0,y=0,"Actual Value",col="red",pch=15)
lines(202:255,f.rnn,col="green",lwd=3)
lines(202:255,f.lasso,col="orange",lwd=3)
plot(f.reg,main="Low Vol Factor Return Prediction")
lines(202:255,ret.test,col="red",lwd=3)
lines(202:255,f.lasso,col="orange",lwd=3)
lines(202:255,f.rf,col="yellow",lwd=3)
lines(202:255,f.rnn,col="green",lwd=3)
legend(x=0,y=1,"Time Series Regression",col="blue",pch=15)
legend(x=0,y=-0.35,"Lasso Regression",col="orange",pch=15)
legend(x=0,y=1-0.35,"Random Forest",col="yellow",pch=15)
legend(x=0,y=1-0.7,"Regularized Neural Net",col="green",pch=15)
legend(x=0,y=0,"Actual Value",col="red",pch=15)
f.reg
colnames(df)
rownames(df)
rownames(df)[(nrow(df)-255):255]
rownames(df)[(nrow(df)-255):nrow(df)]
length(rownames(df)[(nrow(df)-255):nrow(df)])
length(rownames(df)[(nrow(df)-254):nrow(df)])
plot(rownames(df)[(nrow(df)-254):nrow(df)],f.reg,main="Low Vol Factor Return Prediction")
plot(rownames(df)[(nrow(df)-253):nrow(df)],f.reg,main="Low Vol Factor Return Prediction")
plot(rownames(df)[(nrow(df)-255):nrow(df)],f.reg,main="Low Vol Factor Return Prediction")
plot(rownames(df)[(nrow(df)-256):nrow(df)],f.reg,main="Low Vol Factor Return Prediction")
plot(x=rownames(df)[(nrow(df)-256):nrow(df)],y=f.reg,main="Low Vol Factor Return Prediction")
length(f.reg)
?axis
plot(f.reg,main="Low Vol Factor Return Prediction",xlab="")
plot(f.reg,main="Low Vol Factor Return Prediction")
lines(202:255,ret.test,col="red",lwd=3)
lines(202:255,f.lasso,col="orange",lwd=3)
lines(202:255,f.rf,col="yellow",lwd=3)
legend(x=0,y=1,"Time Series Regression",col="blue",pch=15)
lines(202:255,f.rnn,col="green",lwd=3)
legend(x=0,y=-0.35,"Lasso Regression",col="orange",pch=15)
legend(x=0,y=1-0.35,"Random Forest",col="yellow",pch=15)
legend(x=0,y=1-0.7,"Regularized Neural Net",col="green",pch=15)
legend(x=0,y=0,"Actual Value",col="red",pch=15)
?axis
axis(side=1,labels=rownames(df)[(nrow(df)-255):nrow(df)])
?axis
axis(side=1,at=1:255,labels=rownames(df)[(nrow(df)-255):nrow(df)])
axis(side=1,at=1:255,labels=rownames(df)[(nrow(df)-256):nrow(df)])
axis(side=1,at=1:255,labels=rownames(df)[(nrow(df)-254):nrow(df)])
axis(side=1,at=seq(1,255,by=4),labels=rownames(df)[seq(nrow(df)-254,nrow(df),by=4)])
axis(side=1,at=seq(1,255,by=20),labels=rownames(df)[seq(nrow(df)-254,nrow(df),by=20)])
axis(side=1,at=seq(1,255,by=200),labels=rownames(df)[seq(nrow(df)-254,nrow(df),by=20)])
axis(side=1,at=seq(1,255,by=200),labels=rownames(df)[seq(nrow(df)-254,nrow(df),by=200)])
axis(side=1,at=seq(1,255,by=1),labels=rownames(df)[seq(nrow(df)-254,nrow(df),by=1)])
plot(f.reg,main="Low Vol Factor Return Prediction")
lines(202:255,ret.test,col="red",lwd=3)
lines(202:255,f.lasso,col="orange",lwd=3)
lines(202:255,f.rf,col="yellow",lwd=3)
lines(202:255,f.rnn,col="green",lwd=3)
legend(x=0,y=1,"Time Series Regression",col="blue",pch=15)
legend(x=0,y=-0.35,"Lasso Regression",col="orange",pch=15)
legend(x=0,y=1-0.35,"Random Forest",col="yellow",pch=15)
legend(x=0,y=1-0.7,"Regularized Neural Net",col="green",pch=15)
legend(x=0,y=0,"Actual Value",col="red",pch=15)
axis(side=1,at=seq(1,255,by=20),labels=rownames(df)[seq(nrow(df)-254,nrow(df),by=20)])
plot(f.reg,main="Low Vol Factor Return Prediction")
lines(202:255,ret.test,col="red",lwd=3)
lines(202:255,f.lasso,col="orange",lwd=3)
lines(202:255,f.rf,col="yellow",lwd=3)
lines(202:255,f.rnn,col="green",lwd=3)
legend(x=0,y=1,"Time Series Regression",col="blue",pch=15)
legend(x=0,y=-0.35,"Lasso Regression",col="orange",pch=15)
legend(x=0,y=1-0.35,"Random Forest",col="yellow",pch=15)
legend(x=0,y=1-0.7,"Regularized Neural Net",col="green",pch=15)
legend(x=0,y=0,"Actual Value",col="red",pch=15)
axis(side=1,at=seq(1,255,by=60),labels=rownames(df)[seq(nrow(df)-254,nrow(df),by=60)])
?axis
plot(f.reg,main="Low Vol Factor Return Prediction",axes=FALSE)
lines(202:255,ret.test,col="red",lwd=3)
lines(202:255,f.lasso,col="orange",lwd=3)
lines(202:255,f.rf,col="yellow",lwd=3)
lines(202:255,f.rnn,col="green",lwd=3)
legend(x=0,y=1,"Time Series Regression",col="blue",pch=15)
legend(x=0,y=-0.35,"Lasso Regression",col="orange",pch=15)
legend(x=0,y=1-0.35,"Random Forest",col="yellow",pch=15)
legend(x=0,y=1-0.7,"Regularized Neural Net",col="green",pch=15)
legend(x=0,y=0,"Actual Value",col="red",pch=15)
axis(side=1,at=seq(1,255,by=20),labels=rownames(df)[seq(nrow(df)-254,nrow(df),by=20)])
axis(side=1,at=seq(1,255,by=4),labels=rownames(df)[seq(nrow(df)-254,nrow(df),by=4)])
?plot
plot(f.reg,main="Low Vol Factor Return Prediction",axes=FALSE)
lines(202:255,ret.test,col="red",lwd=3)
lines(202:255,f.lasso,col="orange",lwd=3)
lines(202:255,f.rf,col="yellow",lwd=3)
legend(x=0,y=1,"Time Series Regression",col="blue",pch=15)
legend(x=0,y=-0.35,"Lasso Regression",col="orange",pch=15)
legend(x=0,y=1-0.35,"Random Forest",col="yellow",pch=15)
legend(x=0,y=1-0.7,"Regularized Neural Net",col="green",pch=15)
legend(x=0,y=0,"Actual Value",col="red",pch=15)
lines(202:255,f.rnn,col="green",lwd=3)
axis(side=1,at=seq(1,255,by=4),labels=rownames(df)[seq(nrow(df)-254,nrow(df),by=4)])
?axis
axis(side=2,labels=TRUE)
res
round(res,1)
round(resScaled,3)
rec = apply(resScaled,1,vote)
rec
write.csv(resScaled,"tmp.csv")
write.csv(rec,"tmp1.csv")
res
write.csv(res,"tmp2.csv")
apply(res,1,vote)
rec = apply(res,1,vote)
rec = apply(res,1,vote)
write.csv(rec,"tmp1.csv")
Solution = cobyla(x0 = x0_,fn=fn_,hin=hin_,lower=rep(0.0,times=10),upper=rep(0.35,times=10))
require(nloptr)
Solution = cobyla(x0 = x0_,fn=fn_,hin=hin_,lower=rep(0.0,times=10),upper=rep(0.35,times=10))
